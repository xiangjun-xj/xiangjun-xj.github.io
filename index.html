<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    a {
    color: #2171b5;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #d94801;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Roboto', 'Noto Sans SC', sans-serif;
    font-size: 14px;
    font-weight: 400
    }
    .hp-photo{ width:240px; height:240px; border-radius:240px; -webkit-border-radius:240px; -moz-border-radius:240px; }
    strong {
    font-family: 'Roboto', 'Noto Sans SC', sans-serif;
    font-size: 14px;
    font-weight: 600
    }
    heading {
    font-family: 'Roboto', 'Noto Sans SC', sans-serif;
    font-size: 22px;
    font-weight: 600
    }
    papertitle {
    font-family: 'Roboto', 'Noto Sans SC', sans-serif;
    color: 	#084594;
    font-size: 16px;
    font-weight: 600
    }
    institution {
    font-family: 'Roboto', 'Noto Sans SC', sans-serif;
    font-size: 16px;
    font-weight: 600
    }
    name {
    font-family: 'Roboto', 'Noto Sans SC', sans-serif;
    font-size: 32px;
    font-weight: 600
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    </style>

    <title>Jun Xiang</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <!-- <link rel="icon" type="image/jpg" href="./imgs/ustc_icon.png"> -->
</head>

<body>
<table width="840" border="0" align="center" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td>

   
    <!--SECTION 1 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="61.8%" valign="middle">
                <p align="center"><name>Jun Xiang (项 军)</name></p>
                <br><br>Ph.D. student @ USTC / Research assistant @ PolyU
                <br><br>Email: junxiang@mail.ustc.edu.cn
                <br><br>Github: <a href="https://github.com/xiangjun-xj">github.com/xiangjun-xj</a>
                  <p align="justify">I am a fourth-year Ph.D. student in <a href="http://gcl.ustc.edu.cn/">Graphics & Geometric Computing Laboratory (GCL)</a>
                    at <a href="https://en.ustc.edu.cn/">University of Science and Technology of China (USTC)</a> supervised by Prof. <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a>,
                    also a research assistant at <a href="https://www.polyu.edu.hk/">The Hong Kong Polytechnic University (PolyU)</a> guided by Prof. <a href="https://www.polyu.edu.hk/ama/profile/dfsun/">Defeng Sun</a> and Dr. <a href="https://www.polyu.edu.hk/ama/people/academic-staff/dr-yuan-yancheng/">Yancheng Yuan</a>.
                    I obtained my B.S. degree in Mathematics in 2021 from USTC. And I undertook an internship in <a href="https://idr.ai/">Image Derivative Inc</a>. during Mar.2022-Jun.2023.

                    <!-- <br><br><font color="FF4500">I am looking for formal job opportunities in 2024 / 2025 !</font> -->
	            <!-- </br><br>
                </p><p align="center">
                    <a href="https://github.com/RainbowRui">Github <font color="FF8C00">(1.8k+ star)</font></a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=fqoe18wAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://twitter.com/HongruiCai">Twitter</a> &nbsp;/&nbsp;
	                <a href="https://www.linkedin.com/in/hongrui-cai-5a5a12191/">LinkedIn</a>
                </p> -->
                <!-- </p><p align="center">
                    <a href="./CurriculumVitae_HongruiCai.pdf">CV (English)</a> &nbsp;/&nbsp;
                    <a href="./CurriculumVitae_HongruiCai_Chinese.pdf">CV (Chinese)</a>
                </p> -->
              </td>
              <td style="padding:2.5%;width:30%;max-width:30%">
                <a href="./imgs/photo.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="./imgs/photo.jpg" class="hoverZoomLink"></a>
              </td></tr>
			  <!--<td align="right"> <img class="hp-photo" src="./imgs/photo.jpg" style="width: 165;"></td> -->
            </tbody>
          </table>


    <!--SECTION 2 -->
    <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>Research Interests</heading>
		  <p align="justify">My research interests (some papers <span class="highlight">highlighted</span>) include several sub-fields of Computer Vision and Graphics:</p>
<ul>
    <li><p><b>Avatar Modeling and Animation via <em>NeRF</em> and <em>3DGS</em></b></p></li>
    <li><p><b>3D Reconstruction</b> and <b>Structure-from-motion</b></p></li>
    <li><p>Point Cloud Learning and Surface Registration</p></li>
</ul>
</p></br></tr>
       </tbody>
    </table> -->
			
    <!--SECTION 5 -->
    <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td><heading>Publications</heading>
          </td>
          </tr>
		  </tbody>
    </table> -->
    
    
    <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody><tr bgcolor="#ffffd0">
            <td width="30%" align="center"><img src="./imgs/NDR.png" alt="NeurIPS2022" style="width:200px;"></td>
            <td width="70%" valign="center">
                 <p>
                 <papertitle>Neural Surface Reconstruction of Dynamic Scenes with Monocular RGB-D Camera</papertitle>
                 <br><strong>Hongrui Cai</strong>, <a href="https://wanquanf.github.io/">Wanquan Feng</a>,
                 <a href="https://scholar.google.com/citations?hl=en&user=5G-2EFcAAAAJ">Xuetao Feng</a>, Yan Wang,
                 <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a>.<br>
                 <br>
                 <em>NeurIPS</em>, 2022 <strong>(<span style="color:#FF8C00;">Spotlight</span>)</strong><br>
		         <a href="https://ustc3dv.github.io/ndr/">project page</a> /
                 <a href="https://arxiv.org/pdf/2206.15258.pdf">paper</a> /
                 <a href="https://openreview.net/forum?id=8RKJj1YDBJT">OpenReview</a> /
                 <a href="./data/NDR_poster.pdf">poster</a> /
                 <a href="https://github.com/USTC3DV/NDR-code">code</a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=USTC3DV&repo=NDR-code&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px" align="center"></iframe>
                 <p align="justify" style="font-size:13px">We propose Neural-DynamicReconstruction (NDR), a template-free method to recover high-fidelity geometry and motions of a dynamic scene from a monocular RGB-D camera.  To represent and constrain the non-rigid deformations, we propose a novel neural invertible deforming network such that the cycle consistency between arbitrary two frames is automatically satisfied.</p>
                <p></p>
            </td>
    </tr> -->

    <!-- <tbody><tr>
    <td width="30%" align="center"><img src="./imgs/CaricatureFace.jpg" alt="GMOD2021" style="width:200px;"></td>
            <td width="70%" valign="center">
                 <p>
                 <papertitle>Landmark Detection and 3D Face Reconstruction for Caricature using a Nonlinear Parametric Model</papertitle>
                 <br><strong>Hongrui Cai</strong>, <a href="https://yudongguo.github.io/">Yudong Guo</a>, 
                 Zhuang Peng, <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a>.<br>
                 <br>
                 <em>Graphical Models (GMOD)</em>, 2021<br>
                 <a href="https://arxiv.org/pdf/2004.09190.pdf">paper</a> /
                 <a href="https://drive.google.com/file/d/1oUc9XnjBtTJ5PBrIOWTxQrCGoW_LBnCD/view?usp=sharing">slides</a> /
                 <a href="https://zhuanlan.zhihu.com/p/389870247">zhihu</a> /
		         <a href="https://github.com/Juyong/CaricatureFace">code</a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=Juyong&repo=CaricatureFace&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px" align="center"></iframe>
                 <p align="justify" style="font-size:13px">To the best of our knowledge, this is the first work for automatic landmark detection and 3D face reconstruction for general caricatures.</p>
                <p></p>
            </td>
	</tr> -->
    
    <!-- <tbody><tr bgcolor="#ffffd0">
    <td width="30%" align="center"><img src="./imgs/NeuralPoints.png" alt="CVPR2022" style="width:200px;"></td>
            <td width="70%" valign="center">
                 <p>
                 <papertitle>Neural Points: Point Cloud Representation with Neural Fields for Arbitrary Upsampling</papertitle>
                 <br><a href="https://wanquanf.github.io/">Wanquan Feng</a>, Jin Li,
	         <strong>Hongrui Cai</strong>, <a href="https://www.guet.edu.cn/people/info/1003/2398.htm">Xiaonan Luo</a>,
                 <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a>.<br>
                 <br>
                 <em>CVPR</em>, 2022<br>
		         <a href="https://wanquanf.github.io/NeuralPoints">project page</a> /
                 <a href="https://arxiv.org/pdf/2112.04148.pdf">paper</a> /
		         <a href="https://www.zhihu.com/question/519162597/answer/2393206762">zhihu</a> /
		         <a href="https://github.com/WanquanF/NeuralPoints">code</a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=WanquanF&repo=NeuralPoints&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px" align="center"></iframe>
                 <p align="justify" style="font-size:13px">In this paper, we propose Neural Points, a novel point cloud representation. Different from traditional point cloud representation, each point in Neural Points represents a local continuous geometric shape via neural fields. Therefore, Neural Points can express more complex geometry shapes.</p>
                <p></p>
            </td>
	</tr> -->

    <!-- <tbody><tr bgcolor="#ffffd0">
   <td width="30%" align="center"><img src="./imgs/RMA-Net.jpg" alt="CVPR2021" style="width:200px;"></td>
            <td width="70%" valign="center">
                 <p>
                 <papertitle>Recurrent Multi-view Alignment Network for Unsupervised Surface Registration</papertitle>
                 <br><a href="https://wanquanf.github.io/">Wanquan Feng</a>, <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a>, 
                 <strong>Hongrui Cai</strong>, <a href="https://haofeixu.github.io/">Haofei Xu</a>, 
                 <a href="https://sites.google.com/site/junhuihoushomepage/">Junhui Hou</a>, <a href="http://www.cad.zju.edu.cn/home/bao/">Hujun Bao</a>.<br>
                 <br>
                 <em>CVPR</em>, 2021<br>
                 <a href="https://wanquanf.github.io/RMA-Net.html">project page</a> /
                 <a href="https://arxiv.org/pdf/2011.12104.pdf">paper</a> /
                 <a href="https://zhuanlan.zhihu.com/p/380831519">zhihu</a> /
                 <a href="https://github.com/WanquanF/RMA-Net">code</a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=WanquanF&repo=RMA-Net&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px" align="center"></iframe>
                 <p align="justify" style="font-size:13px">For non-rigid registration, we propose RMA-Net to deform the input surface shape stage by stage. RMA-Net is trained in an unsupervised manner via our proposed multi-view 2D projection loss.</p>
                <p></p>
            </td>
        </tr> -->

    <!-- <tbody><tr>
        <td width="30%" align="center"><img src="./imgs/Cari2D3D.png" alt="submission" style="width:200px;"></td>
            <td width="70%" valign="center">
                 <papertitle>Double References Guided Interactive 2D and 3D Caricature Generation</papertitle>
                 <br>Xin Huang, Dong Liang, <strong>Hongrui Cai</strong>, Yunfeng Bai, 
                 <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a>, Feng Tian, Jinyuan Jia.<br>
                 <br>
                 <em>ACM TOMM</em>, 2024<br>
                 <a href="https://dl.acm.org/doi/pdf/10.1145/3655624">paper</a>
                 <p align="justify" style="font-size:13px">We propose the first geometry and texture (double) referenced interactive 2D and 3D caricature generating and editing method.</p>
                <p></p>
            </td>		
        </tr> -->

    <!-- <tbody><tr>
    <td width="30%" align="center"><img src="./imgs/DGNR.png" alt="CIMS2023" style="width:200px;"></td>
        <td width="70%" valign="center">
                <papertitle>Differentiable Deformation Graph based Neural Non-rigid Registration</papertitle>
                <br><a href="https://wanquanf.github.io/">Wanquan Feng</a>, <strong>Hongrui Cai</strong>,
                <a href="https://sites.google.com/site/junhuihoushomepage/">Junhui Hou</a>, <a href="http://www.bdeng.me/">Bailin Deng</a>, 
                <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a>.<br>
                <br>
                <em>Communications in Mathematics and Statistics (CIMS)</em>, 2023<br>
                <a href="https://link.springer.com/article/10.1007/s40304-023-00341-x">paper</a> /
                <a href="data/DGNR.mp4">video</a>
            
                <p align="justify" style="font-size:13px">We propose a differentiable deformation graph based neural learning method for non-rigid registration by replacing some components with neural based strategies to fully take advantage of the shape priors and domain knowledge embedded in trained neural networks.</p>
            <p></p>
        </td>		
    </tr> -->

    <!-- <tbody><tr>
    <td width="30%" align="center"><img src="./imgs/CariPainter.png" alt="ACMMM2022" style="width:200px;"></td>
            <td width="70%" valign="center">
                 <p>
                 <papertitle>CariPainter: Sketch Guided Interactive Caricature Generation</papertitle>
                 <br>Xin Huang, Dong Liang, <strong>Hongrui Cai</strong>,
                 <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a>, Jinyuan Jia.<br>
                 <br>
                 <em>ACM MM</em>, 2022<br>
                 <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548085">paper & video</a>
                 <p align="justify" style="font-size:13px">We propose CariPainter, the first interactive caricature generating and editing method, by utilizing the semantic segmentation maps as an intermediary domain.</p>
                <p></p>
            </td>
    </tr> -->


	<!--SECTION 7 -->
        <!-- <tbody><tr>
		<td width="30%" align="center"><img src="./imgs/PerspectiveCorrection.jpg" alt="CVM2021" style="width:200px;"></td>
            <td width="70%" valign="center">
                 <p>
                 <papertitle>Real-time Face View Correction for Front-facing Cameras</papertitle>
                 <br><a href="https://yudongguo.github.io/">Yudong Guo</a>, <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a>, 
                 Yihua Chen, <strong>Hongrui Cai</strong>, <a href="http://staff.ustc.edu.cn/~zhuang/">Zhangjin Huang</a>, 
                 <a href="http://www.bdeng.me/">Bailin Deng</a>.<br>
                 <br>
                 <em>Computational Visual Media (CVM)</em>, 2021<br>
                 <a href="https://link.springer.com/content/pdf/10.1007/s41095-021-0215-y.pdf">paper</a> /
		         <a href="data/FaceViewCorrection.mp4">video</a>
				
                 <p align="justify" style="font-size:13px">We take the video stream from a single RGB camera as input, and generate a video stream that emulates the view from a virtual camera at a designated location.</p>
                <p></p>
            </td>
        </tr>
	
        </tbody>
    </table> -->
	

    <!--SECTION 8 -->
    <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td><heading>Preprints</heading>
          </td>
          </tr>
		  </tbody>
    </table> -->

    <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
    <td width="30%" align="center"><img src="./imgs/hks.png" alt="submission" style="width:200px;"></td>
            <td width="70%" valign="center">
                 <papertitle>Self-Supervised Topology-Aware Non-Rigid Point Cloud Registration</papertitle>
                 <br>Zhuang Peng*, <strong>Hongrui Cai*</strong>,
                 <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a> (*equal contribution).<br>
                 <p align="justify" style="font-size:13px">We propose a topology-aware module to perceive topological changes in motion to improve the performance of the algorithm in the dynamic topology region.</p>
                <p></p>
            </td>		
        </tr> -->
    
    <!-- <tbody><tr>
    <td width="30%" align="center"><img src="./imgs/GauMesh.jpg" alt="submission" style="width:200px;"></td>
            <td width="70%" valign="center">
                 <papertitle>Bridging 3D Gaussian and Mesh for Freeview Video Rendering</papertitle>
                 <br><a href="https://svip-lab.github.io/team/xiaoyt.html">Yuting Xiao</a>, <a href="https://xuanwangvc.github.io/">Xuan Wang</a>,
                 Jiafei Li, <strong>Hongrui Cai</strong>, <a href="https://sites.google.com/site/yanbofan0124/">Yanbo Fan</a>,
                 <a href="https://xuenan.net/">Nan Xue</a>, Minghui Yang, <a href="https://shenyujun.github.io/">Yujun Shen</a>,
                 <a href="https://scholar.google.com.sg/citations?user=fe-1v0MAAAAJ&hl=en">Shenghua Gao</a>.<br>
                 <br>
                 <a href="https://arxiv.org/pdf/2403.11453">paper</a>
                 <p align="justify" style="font-size:13px">We propose a novel approach to bridge 3DGS and mesh for modeling and rendering the dynamic scenes.</p>
                <p></p>
            </td>		
        </tr>

        </tbody>
    </table> -->


    <!--SECTION 9 -->
    <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td><heading>Technical Projects</heading>
          </td>
          </tr>
		  </tbody>
    </table> -->

    <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
	<td width="30%" align="center"><img src="./imgs/LD-FER.png" alt="project" style="width:200px;"></td>
            <td width="70%" valign="center">
                 <papertitle>Landmark driven Facial Expression Recognition</papertitle>
                 <br><strong>Hongrui Cai</strong>.<br>
                 <br>
                 2020
                 <br>
                 <a href="https://github.com/RainbowRui/Landmark-Driven-Facial-Expression-Recognition">code</a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=RainbowRui&repo=Landmark-Driven-Facial-Expression-Recognition&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px" align="center"></iframe>
				
                 <p align="justify" style="font-size:13px">A landmark-driven Facial Expression Recognition (FER) method without employing any pre-trained models from other tasks</p>
                <p></p>
            </td>		
        </tr>
			
        </tbody>
    </table> -->


    <!--SECTION 9 -->
    <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td><heading>Experiences</heading>
          </td>
          </tr>
		  </tbody>
    </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody><tr>

            <td width="30%" align="center"><img src="./imgs/ant_icon.png" alt="project" style="width:90px"></td>
                <td width="70%" valign="center">
                <institution>Ant Group</institution></div><br><br>
                <div class="period">May. 2023 - Present</div><br>
                <div class="position">Research Intern @ Interaction Intelligence Lab</div>
              </td>
            </tr>

            <td width="30%" align="center"><img src="./imgs/ustc_icon.png" alt="project" style="width:100px"></td>
                <td width="70%" valign="center">
                <institution>University of Science and Technology of China</institution></div><br><br>
                <div class="period">Sep. 2019 - Present</div><br>
                <div class="position">Ph.D. candidate in Data Science</div>
              </td>
            </tr>

            <td width="30%" align="center"><img src="./imgs/scut_icon.jpeg" alt="project" style="width:100px"></td>
                <td width="70%" valign="center">
                <institution>South China University of Technology</institution></div><br><br>
                <div class="period">Sep. 2015 - Jun. 2019</div><br>
                <div class="position">Bachelor Degree in Mathematics and Applied Mathematics</div>
              </td>
            </tr>
			
        </tbody>
    </table> -->


    <!--SECTION 10 -->
    <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><heading>Professional Activities</heading>
             <p align="justify">Conference Reviewers of CVPR, ICCV, ECCV, SIGGRAPH, GMP, AAAI.</p>
             <p align="justify">Journal Reviewers of IEEE TPAMI, IEEE TMM, IEEE CGA, C&G.</p>
            </td>
            </tr></tbody>
    </table> -->


    <!--SECTION 11 -->
    <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><heading>Academic Talks</heading>
             <p align="justify">[2022.12] Invited talk about <a href="https://neurips.cc/virtual/2022/session/64757">NDR</a> at <a href="https://neurips.cc/Conferences/2022">NeurIPS 2022</a>.</p>
             <p align="justify">[2022.11] Invited talk about <a href="https://sota.jiqizhixin.com/project/ndr">NDR</a> at <a href="https://sota.jiqizhixin.com/">Jiqizhixin</a>.</p>
             <p align="justify">[2021.04] Invited talk about <a href="https://drive.google.com/file/d/1oUc9XnjBtTJ5PBrIOWTxQrCGoW_LBnCD/view?usp=sharing">Caricature Face</a> at <a href="http://iccvm.org/2021/home.htm">CVM 2021</a>.</p>
            </td>
            </tr></tbody>
    </table> -->

    <!--SECTION 12 -->
    <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><heading>Selected Honors</heading>
             <p align="justify"> Excellent Graduate Student, by USTC, 2024.</p>
             <p align="justify"> First-class Academic Scholarships for Postgraduates, by USTC, 2019 - 2023.</p>
             <p align="justify"> Excellent Undergraduate Thesis Award, by SCUT, 2019.</p>
             <p align="justify"> Excellent Undergraduate Student, by SCUT, 2019.</p>
            </td>
            </tr></tbody>
    </table> -->

    <!--SECTION 13 -->
    <!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=a8ddb5&w=500&t=tt&d=OXnFLPaDhY6mE_z76oEOuPh3N8PzQ7N3YNPrjF7lemU&co=43a2ca&cmo=fdae6b&cmn=e6550d'></script> -->

    <!--SECTION 14 -->
    <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><br>
		       <p align="right"><font size="2"> Last update: 2024.07</font></p>
            </td>
         </tr>
         </tbody>
     </table> -->


</td>
</tr>
</tbody>
</table>
</body>
</html>