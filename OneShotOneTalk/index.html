<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="One Shot, One Talk: Whole-body Talking Avatar from a Single Image">
  <meta name="keywords" content="Talking Avatar, One-Shot, Diffusion Prior, 3DGS">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- <meta property="og:image" content="./static/images/thumbnail.png"/>
  <link rel="image_src" href="./static/images/thumbnail.png">
  <link rel="icon"
        type="image/x-icon"
        href="./static/images/favicon.ico"/> -->

  <title>One Shot, One Talk: Whole-body Talking Avatar from a Single Image</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-EDF010G6PN"></script>
  <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-EDF010G6PN');


  </script>

  <script type="module"
          src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script type="text/javascript" src="https://code.jquery.com/jquery-1.11.0.min.js"></script>
  <script type="text/javascript" src="https://code.jquery.com/jquery-migrate-1.2.1.min.js"></script>
  <script src="https://unpkg.com/interactjs/dist/interact.min.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" type="text/css" href="./static/slick/slick.css"/>
  <link rel="stylesheet" type="text/css" href="./static/slick/slick-theme.css"/>

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <!-- <div class="column is-4 has-text-centered">
          <img src="static/images/logo.svg" alt="HyperNeRF"/>
        </div> -->
      </div>
      <div class="container has-text-centered">
        <h1 class="title is-1 publication-title">
          One Shot, One Talk: Whole-body Talking<br/>Avatar from a Single Image
        </h1>
        <div class="is-size-5 publication-authors">
          <div class="author-block">
            <!-- <div class="author-portrait">
              <img src="./static/images/portraits/keunhong_rgb.128.gif" class="rgb preload" />
              <img src="./static/images/portraits/keunhong_depth.128.gif" class="depth preload" />
            </div> -->
            <a href="https://xiangjun-xj.github.io/">Jun Xiang</a><sup>1,2</sup>,</div>
          <div class="author-block">
            <!-- <div class="author-portrait">
              <img src="./static/images/portraits/utsinh_rgb.128.gif" class="rgb preload" />
              <img src="./static/images/portraits/utsinh_depth.128.gif" class="depth preload" />
            </div> -->
            <a href="https://yudongguo.github.io/">Yudong Guo</a><sup>1</sup>,</div>
          <div class="author-block">
            <!-- <div class="author-portrait">
              <img src="./static/images/portraits/hedman_rgb.128.gif" class="rgb preload" />
              <img src="./static/images/portraits/hedman_depth.128.gif" class="depth preload" />
            </div> -->
            Leipeng Hu<sup>1</sup>,</div>
          <div class="author-block">
            <!-- <div class="author-portrait">
              <img src="./static/images/portraits/barron_rgb.128.gif" class="rgb preload" />
              <img src="./static/images/portraits/barron_depth.128.gif" class="depth preload" />
            </div> -->
            Boyang Guo<sup>1</sup>,
          </div>
          <div class="author-block">
            <!-- <div class="author-portrait">
              <img src="./static/images/portraits/sofien_rgb.128.gif" class="rgb preload" />
              <img src="./static/images/portraits/sofien_depth.128.gif" class="depth preload" />
            </div> -->
            <a href="https://paulyuanmath.github.io/">Yancheng Yuan</a><sup>2</sup>,
          </div>
          <div class="author-block">
            <!-- <div class="author-portrait">
              <img src="./static/images/portraits/dgo_rgb.128.gif" class="rgb preload" />
              <img src="./static/images/portraits/dgo_depth.128.gif" class="depth preload" />
            </div> -->
            <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a><sup>1</sup>
          </div>
        </div>

        <div class="is-size-5 publication-authors">
          <span class="author-block"><sup>1</sup>University of Science and Technology of China,</span>
          <span class="author-block"><sup>2</sup>The Hong Kong Polytechnic University</span>
        </div>

        <div class="column has-text-centered">
          <div class="publication-links">
            <!-- PDF Link. -->
            <span class="link-block">
                <a href="https://xiangjun-xj.github.io/OneShotOneTalk/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (coming soon)</span>
                </a>
              </span>
            <span class="link-block">
                <a href="https://xiangjun-xj.github.io/OneShotOneTalk/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (coming soon)</span>
                </a>
            </span>
            <span class="link-block">
              <a href="https://xiangjun-xj.github.io/OneShotOneTalk/"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code (coming soon)</span>
                </a>
            </span>
            <!-- Dataset Link. -->
            <!-- <span class="link-block">
              <a href="https://github.com/google/hypernerf/releases/tag/v0.1"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="far fa-images"></i>
                </span>
                <span>Data</span>
                </a>
              </span> -->
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <video id="teaser" autoplay controls muted loop playsinline height="100%">
        <!-- <source src="./static/images/teaser-crop.mp4" type="video/mp4"> -->
        <source src="https://raw.githubusercontent.com/xiangjun-xj/project_page_assets/master/OneShotOneTalk/teaser-crop.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle">
      <!-- <h2> -->
        Given a one-shot image (e.g., your favorite photo) as input, our method reconstructs a fully expressive whole-body talking avatar
        that captures personalized details and supports realistic animation, including vivid body gestures and natural expression changes.
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-dark is-small">
  <div class="hero-body">
    <div class="container has-text-centered">
      <div id="results-carousel" class="carousel results-carousel">

        <div>
          <div class="results-item">
            <video poster="" id="espresso-rgb" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/espresso_rgb.mp4"
                      type="video/mp4">
            </video>
            <video poster="" id="espresso-depth" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/espresso_depth.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>

        <div>
          <div class="results-item">
            <video poster="" id="split-cookie-rgb" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/split-cookie_rgb.mp4"
                      type="video/mp4">
            </video>
            <video poster="" id="split-cookie-depth" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/split-cookie_depth.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>

        <div>
          <div class="results-item">
            <video poster="" id="3dprinter-rgb" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/3dprinter_rgb.mp4"
                      type="video/mp4">
            </video>
            <video poster="" id="3dprinter-depth" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/3dprinter_depth.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>

        <div>
          <div class="results-item">
            <video poster="" id="ricardo-rgb" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/ricardo_rgb.mp4"
                      type="video/mp4">
            </video>
            <video poster="" id="ricardo-depth" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/ricardo_depth.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>

        <div>
          <div class="results-item">
            <video poster="" id="americano-rgb" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/americano_rgb.mp4"
                      type="video/mp4">
            </video>
            <video poster="" id="americano-depth" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/americano_depth.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>

      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-three-quarters">
          <p>
            Here we show results generated with <i>HyperNeRF</i>. These videos show the input video
            being
            played
            back with a stabilized novel camera path. The right side video shows the depth of the
            scene.
            Click on the arrows or drag to see more results.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Building realistic and animatable avatars still requires minutes of multi-view or monocular self-rotating videos,
            and most methods lack precise control over gestures and expressions. To push this boundary,
            we address the challenge of constructing a whole-body talking avatar from a single image.
            We propose a novel pipeline that tackles two critical issues: 1) complex dynamic modeling and 2)
            generalization to novel gestures and expressions. To achieve seamless generalization, we leverage recent
            pose-guided image-to-video diffusion models to generate imperfect video frames as pseudo-labels.
            To overcome the dynamic modeling challenge posed by inconsistent and noisy pseudo-videos, we introduce a tightly
            coupled 3DGS-mesh hybrid avatar representation and apply several key regularizations to mitigate inconsistencies
            caused by imperfect labels. Extensive experiments on diverse subjects demonstrate that our method enables
            the creation of a photorealistic, precisely animatable, and expressive whole-body talking avatar from just a single image.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2">Video</h2>
        <div class="publication-video">
          <iframe width="640" height="480" src="https://www.youtube.com/embed/qzgdE_ghkaI"
                  title="YouTube video player" frameborder="0"
                  allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                  allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" style="margin-top: -20px">Method Overview</h2>

        <img src="https://raw.githubusercontent.com/xiangjun-xj/project_page_assets/master/OneShotOneTalk/pipe.png" class="center">
        <div class="content has-text-justified">
          <p style="margin-top: 30px">
            Our method constructs an expressive whole-body talking avatar from a single image.
            We begin by generating pseudo body and head frames using pre-trained generative models,
            driven by a collected video dataset with diverse poses. Per-pixel supervision on the input image,
            perceptual supervision on imperfect pseudo labels, and mesh-related constraints are then applied to guide the 3DGS-mesh
            coupled avatar representation, ensuring realistic and expressive avatar reconstruction and animation.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="https://raw.githubusercontent.com/xiangjun-xj/project_page_assets/master/OneShotOneTalk/pipeline-crop.mp4"
                    type="video/mp4">
          </video>
        </div>

      </div>
    </div>
    <!--/ Animation. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Results. -->
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Comparison</h2>

        <div class="content has-text-justified">
          <p>
            We compare our work with baseline methods, ExAvatar, ELICIT, MimicMotion and Make-Your-Anchor on diverse tasks like
            cross-identity motion reenactment, self-driven motion reenactment and upper-body video-driven.
            Our method achieves accurate and realistic animation with almost all fine details preserved and identity unchanged.          
          </p>
          <p></p>
            Note that for ExAvatar, since it takes short videos as input, we compare with two versions of it:
            ExAvatar-40shot, which uses 40-shot images, and ExAvatar-1shot, which uses one-shot images as input.
            For Make-Your-Anchor, as we find it does not perform well on one-shot input, we only compare it with available
            video input by fine-tuning it on a short video clip.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="https://raw.githubusercontent.com/xiangjun-xj/project_page_assets/master/OneShotOneTalk/comparison-crop.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- More results. -->
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">More results on cross-identity video-driven
        </h2>

        <div class="content has-text-justified">
          <p>
            Using the same driving pose, identities with completely different attributes can be driven in the same way,
            thanks to the SMPL-X model and the 3DGS-mesh coupled representation.          
          </p>
        </div>
        
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="https://raw.githubusercontent.com/xiangjun-xj/project_page_assets/master/OneShotOneTalk/samepose-crop.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- More results. -->
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">More results on NVS and motion reenactment
        </h2>
        
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="https://raw.githubusercontent.com/xiangjun-xj/project_page_assets/master/OneShotOneTalk/more-crop.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>

    <div class="content has-text-justified">
      <p>
        In this paper, we introduce a novel pipeline for creating expressive talking avatars from a single image.
        We propose a coupled 3DGS-Mesh avatar representation, incorporating several key constraints and a carefully designed hybrid
        learning framework that combines information from both the input image and pseudo frames.
        Experimental results demonstrate that our method outperforms existing techniques, with our one-shot avatar even surpassing
        state-of-the-art methods that require video input. Considering its simplicity in construction and ability to generate vivid,
        realistic animations, our method shows significant potential for practical applications of talking avatars across various fields.
      </p>
    </div>

  </div>
</section>

<hr/>



<!-- <section class="section" id="BibTeX">
  <div class="container content is-max-desktop">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021hypernerf,
  author = {Park, Keunhong and Sinha, Utkarsh and Hedman, Peter and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Martin-Brualla, Ricardo and Seitz, Steven M.},
  title = {HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields},
  journal = {ACM Trans. Graph.},
  issue_date = {December 2021},
  publisher = {ACM},
  volume = {40},
  number = {6},
  month = {dec},
  year = {2021},
  articleno = {238},
}</code></pre>
  </div>
</section> -->


<!-- <section class="section" id="acknowledgements">
  <div class="container content is-max-desktop">
    <h2 class="title">Acknowledgements</h2>
    <p>Special thanks to <a href="https://homes.cs.washington.edu/~holynski/">Aleksander Hołyński</a>,
      <a href="https://roxanneluo.github.io/">Xuan Luo</a>, and Haley Cho for their support and
      help with collecting data. Thanks to <a href="https://zhengqili.github.io/">Zhengqi Li</a> and
      <a href="http://www.oliverwang.info/">Oliver Wang</a> for their help with the NSFF experiments.</p>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://xiangjun-xj.github.io/OneShotOneTalk/">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://xiangjun-xj.github.io/OneShotOneTalk/" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
  </div>
</footer>

<script type="text/javascript" src="./static/slick/slick.min.js"></script>
</body>
</html>
